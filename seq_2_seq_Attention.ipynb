{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import unicodedata\n",
    "import string\n",
    "import os \n",
    "import pandas as pd\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To enable gpu in the device.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create index2char & char2index dictionary.\n",
    "# This function does preprocessing of train, test and validation path and create pairs of english & hindi words.\n",
    "def preprocessData(currdir, lang_chosen, index2char, char2index, data_type = 'train'):\n",
    "    train_path = os.path.join(currdir, lang_chosen, lang_chosen + '_train.csv')\n",
    "    val_path = os.path.join(currdir, lang_chosen, lang_chosen + '_valid.csv')\n",
    "    test_path = os.path.join(currdir,  lang_chosen, lang_chosen + '_test.csv')\n",
    "    if data_type == \"test\":\n",
    "        path = test_path\n",
    "    elif data_type == \"val\":\n",
    "        path = val_path\n",
    "    else:\n",
    "        path = train_path\n",
    "        \n",
    "    data = pd.read_csv(path, names=['input', 'output'])\n",
    "    input = data['input'].to_list()\n",
    "    output = data['output'].to_list()\n",
    "    pair_list =  []\n",
    "    # Create pairs of words.\n",
    "    for i in range(len(input)):\n",
    "        pair = (input[i], output[i])\n",
    "        pair_list.append(pair)\n",
    "        \n",
    "    # Tokens \n",
    "    # 0 -> SOS\n",
    "    # 1 -> EOS\n",
    "    # 3 -> Pad\n",
    "    index2char1 = {0:'<', 1: '>', 2 : '.'}\n",
    "    char2index1 = {'<' : 0, '>' : 1, '.' : 2 }\n",
    "    char_count = {}\n",
    "    num_char = 3\n",
    "    index = 3\n",
    "    maxlength_input = 0\n",
    "    maxlength_output = 0\n",
    "    # Creating char dictionary.\n",
    "    for word in input:\n",
    "        maxlength_input = max(maxlength_input, len(word))\n",
    "        for char in word: \n",
    "            if char not in  char2index1:\n",
    "                char2index1[char] = len(char2index1)\n",
    "                char_count[char] = 1\n",
    "                index2char1[len(index2char1)] = char\n",
    "                index = index + 1\n",
    "            else: \n",
    "                char_count[char] = char_count[char] + 1\n",
    "                \n",
    "    for word in output:\n",
    "        maxlength_output = max(maxlength_output, len(word))\n",
    "        for char in word: \n",
    "            if char not in  char2index1:\n",
    "                char2index1[char] = len(char2index1)\n",
    "                char_count[char] = 1\n",
    "                index2char1[len(char2index1)] = char\n",
    "                index = index + 1\n",
    "            else: \n",
    "                char_count[char] = char_count[char] + 1\n",
    "    # Adding in the main index2char and char2index dictionary\n",
    "    for word in input:\n",
    "        for char in word: \n",
    "            if char not in  char2index:\n",
    "                char2index[char] = len(char2index)\n",
    "                index2char[len(index2char)] = char\n",
    "                \n",
    "    for word in output:\n",
    "        for char in word: \n",
    "            if char not in  char2index:\n",
    "                char2index[char] = len(char2index)\n",
    "                index2char[len(index2char)] = char\n",
    "    return char_count, char2index, index2char, maxlength_input, maxlength_output, pair_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a vector for the word which contains its indices from char2index dictionary.\n",
    "def word2vec(char2index, word):\n",
    "    vec = []\n",
    "    for char in word:\n",
    "        vec.append(char2index[char])\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor for each of the words containg PAD_token, SOS_token & EOS_token and rest of the indices.\n",
    "def create_vec(char2index, maxlength, word):\n",
    "    wordvec = word2vec(char2index, word)\n",
    "    wordvec.append(EOS_token)\n",
    "    for i in range(maxlength - len(word)):\n",
    "        wordvec.append(PAD_token)\n",
    "    wordvec = torch.LongTensor(wordvec)\n",
    "    return wordvec\n",
    "\n",
    "# Function to create a pair of tensors of embedding of english & hindi words in a pair.\n",
    "def create_vec_pair(char2index, maxlength, pair_list):\n",
    "    vec_pair_list = []\n",
    "    for word_pair in pair_list:\n",
    "        eng_vec = create_vec(char2index, maxlength, word_pair[0])\n",
    "        hind_vec = create_vec(char2index, maxlength, word_pair[1])\n",
    "        vec_pair = (eng_vec, hind_vec)\n",
    "        vec_pair_list.append(vec_pair)\n",
    "    return vec_pair_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuction to calculate the average accuracy and average loss of the trained model.\n",
    "def evaluate(encoder, decoder, loader, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size, criterion, max_length, index2char):\n",
    "    loss = total = correct = 0    \n",
    "    with torch.no_grad():\n",
    "        # for each of the batches in the loader, checking if each of the word is completely matching\n",
    "        #  with the predicted word or not.\n",
    "     \n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_loss = 0\n",
    "\n",
    "            encoder_hidden = encoder.initHidden(num_layers_encoder)\n",
    "            if cell_type == \"LSTM\":\n",
    "                encoder_cell_state = encoder.initHidden(num_layers_encoder)\n",
    "                encoder_hidden = (encoder_hidden, encoder_cell_state)\n",
    "\n",
    "            # Transforming input & target variable to extract each of the letter from the words of batches.\n",
    "            input_variable = batch_x.transpose(0, 1)\n",
    "            output_variable = batch_y.transpose(0, 1)\n",
    "\n",
    "            input_length = input_variable.size(0)\n",
    "            target_length = output_variable.size(0)\n",
    "\n",
    "            output = torch.LongTensor(target_length, batch_size)\n",
    "\n",
    "            encoder_outputs = torch.zeros(max_length, batch_size, encoder.hidden_size)\n",
    "            encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "            \n",
    "            decoder_input = torch.LongTensor([SOS_token] * batch_size)\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            for i in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(input_variable[i], encoder_hidden)\n",
    "            # passing the last output of the encoder to the fist cell of decoder. \n",
    "            decoder_hidden = encoder_hidden\n",
    "        \n",
    "            for j in range(target_length):\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "                batch_loss += criterion(decoder_output, output_variable[j].squeeze())\n",
    "\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                decoder_input = torch.cat(tuple(topi))\n",
    "                output[j] = torch.cat(tuple(topi))\n",
    "\n",
    "            output = output.transpose(0, 1)\n",
    "\n",
    "            for k in range(output.size(0)):\n",
    "                to_ignore = [SOS_token, EOS_token, PAD_token]\n",
    "                pred = []\n",
    "                y = []\n",
    "                for w in output[k]: \n",
    "                    if w not in to_ignore:\n",
    "                        y.append(index2char[w.item()])\n",
    "                \n",
    "                for w in batch_y[k]: \n",
    "                    if w not in to_ignore: \n",
    "                        pred.append(index2char[w.item()])\n",
    "                if y == pred:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "            # computing the average accuracy and loss.\n",
    "            accuracy = (correct / total) * 100\n",
    "            loss += batch_loss.item() / target_length\n",
    "            \n",
    "    # returning accuracy and loss.\n",
    "    return accuracy, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the words for each of the input in the loader and save the result in a csv file prediction.csv\n",
    "def inference(encoder, decoder, loader, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size, max_length, index2char):\n",
    "    loss = total = correct = 0    \n",
    "    predictions = {\n",
    "        \"input\" : [], \n",
    "        \"pred\" : [],\n",
    "        \"output\" : []\n",
    "    }\n",
    "    # torch.no_grad() is used to not change the weights and predict the words from the encoder-decoder model.\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            encoder_hidden = encoder.initHidden(num_layers_encoder)\n",
    "            if cell_type == \"LSTM\":\n",
    "                encoder_cell_state = encoder.initHidden(num_layers_encoder)\n",
    "                encoder_hidden = (encoder_hidden, encoder_cell_state)\n",
    "\n",
    "            # used to store all the input words in the given batch_x\n",
    "            input_words = [] \n",
    "            to_ignore = [SOS_token, EOS_token, PAD_token]\n",
    "            for k in batch_x:\n",
    "                input_word = \"\"\n",
    "                for kk in k: \n",
    "                    if kk not in to_ignore:\n",
    "                        input_word = input_word + index2char[kk.item()]\n",
    "                input_words.append(input_word)\n",
    "            input_variable = batch_x.transpose(0, 1)\n",
    "            output_variable = batch_y.transpose(0, 1)\n",
    "\n",
    "            input_length = input_variable.size(0)\n",
    "            target_length = output_variable.size(0)\n",
    "\n",
    "            output = torch.LongTensor(target_length, batch_size)\n",
    "\n",
    "            encoder_outputs = torch.zeros(max_length, batch_size, encoder.hidden_size)\n",
    "            encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "            \n",
    "            decoder_input = torch.LongTensor([SOS_token] * batch_size)\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            for i in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(input_variable[i], encoder_hidden)\n",
    "\n",
    "            decoder_hidden = encoder_hidden\n",
    "        \n",
    "            for j in range(target_length):\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                decoder_input = torch.cat(tuple(topi))\n",
    "                output[j] = torch.cat(tuple(topi))\n",
    "\n",
    "            output = output.transpose(0, 1)\n",
    "            # used to store all the output words and predicted words in a list from the given batch.\n",
    "            output_words = []\n",
    "            pred_words = []\n",
    "            for k in range(output.size(0)):                \n",
    "                pred_word = \"\"\n",
    "                output_word = \"\"\n",
    "                # storing the predicted word.\n",
    "                for w in output[k]: \n",
    "                    if w not in to_ignore:\n",
    "                        pred_word = pred_word + index2char[w.item()]\n",
    "                \n",
    "                for w in batch_y[k]: \n",
    "                    if w not in to_ignore: \n",
    "                        output_word = output_word + index2char[w.item()]\n",
    "                output_words.append(output_word)\n",
    "                pred_words.append(pred_word)\n",
    "        # storing all the words from the current batch in the predictions dictionary. \n",
    "        predictions[\"input\"] = predictions[\"input\"] + input_words\n",
    "        predictions[\"pred\"]= predictions[\"pred\"] + pred_words\n",
    "        predictions[\"output\"] = predictions[\"output\"] + output_words\n",
    "\n",
    "    predict = pd.DataFrame(predictions)\n",
    "    predict.to_csv(\"prediction.csv\")\n",
    "    return predict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN_Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size, input_size):\n",
    "        super(EncoderRNN_Attention, self).__init__()\n",
    "         # Taking the values of all the hyperparameters from the input.\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.cell_type = cell_type\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(input_size, self.embedding_size)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.num_layers_encoder = num_layers_encoder\n",
    "\n",
    "        # For RNN\n",
    "        self.cell_layer = nn.RNN(self.embedding_size, self.hidden_size, num_layers = self.num_layers_encoder, dropout = self.drop_out, bidirectional = self.bidirectional)\n",
    "\n",
    "        # For GRU and LSTM model.   \n",
    "        if self.cell_type == 'GRU':\n",
    "            self.cell_layer = nn.GRU(self.embedding_size, self.hidden_size, num_layers = self.num_layers_encoder, dropout = self.drop_out, bidirectional = self.bidirectional)\n",
    "        elif self.cell_type == 'LSTM':\n",
    "            self.cell_layer = nn.LSTM(self.embedding_size, self.hidden_size, num_layers = self.num_layers_encoder, dropout = self.drop_out, bidirectional = self.bidirectional)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output = self.dropout(embedded.view(1,self.batch_size, -1))\n",
    "        output, hidden = self.cell_layer(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self , num_layers):\n",
    "        if (self.bidirectional == False):\n",
    "            res = torch.zeros(num_layers, self.batch_size, self.hidden_size)\n",
    "        else:\n",
    "            res = torch.zeros(num_layers*2, self.batch_size, self.hidden_size)\n",
    "        res.to(device)\n",
    "        return res\n",
    "\n",
    "class DecoderRNN_Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size, output_size):\n",
    "        super(DecoderRNN_Attention, self).__init__()\n",
    "        # Taking the values of all the hyperparameters from the input.\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell_type = cell_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(output_size, self.embedding_size)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.num_layers_decoder = num_layers_decoder\n",
    "        # For RNN\n",
    "        self.cell_layer = nn.RNN(self.embedding_size + self.hidden_size, self.hidden_size, num_layers = self.num_layers_decoder, dropout = self.dropout, bidirectional = self.bidirectional)\n",
    "\n",
    "        if self.cell_type == 'GRU':\n",
    "            self.cell_layer = nn.GRU(self.embedding_size + self.hidden_size, self.hidden_size, num_layers = self.num_layers_decoder, dropout = self.dropout, bidirectional = self.bidirectional)\n",
    "        elif self.cell_type == 'LSTM':\n",
    "            self.cell_layer = nn.LSTM(self.embedding_size + self.hidden_size, self.hidden_size, num_layers = self.num_layers_decoder, dropout = self.dropout, bidirectional = self.bidirectional)\n",
    "       \n",
    "        # Attention layer\n",
    "        self.attn = nn.Linear(self.hidden_size + self.hidden_size, self.hidden_size)\n",
    "        self.v = nn.Linear(self.hidden_size, 1, bias=False)\n",
    "\n",
    "        if (self.bidirectional == True):\n",
    "            self.out = nn.Linear(2 * self.hidden_size , output_size)\n",
    "        else:\n",
    "            self.out = nn.Linear(self.hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # Compute attention scores\n",
    "        attn_scores = torch.tanh(self.attn(torch.cat((hidden[0], encoder_outputs[0]), dim=1)))\n",
    "        attn_weights = torch.softmax(self.v(attn_scores), dim=1)\n",
    "\n",
    "        # Compute context vector\n",
    "        context = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        # Concatenate input and context vector\n",
    "        input_combined = torch.cat((self.embedding(input).view(1, self.batch_size, -1), context), dim=2)\n",
    "\n",
    "        # Pass through decoder cell layer\n",
    "        output, hidden = self.cell_layer(input_combined, hidden)\n",
    "\n",
    "        # Compute output and return\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "    # Function to create a hidden layer of of size (2*num_layers_decoder) x batch_size x hidden_size in case of bidirectional.\n",
    "    # and of size num_layers_decoder x batch_size x hidden_size in case of non bidirectional model.\n",
    "    def initHidden(self):\n",
    "        if (self.bidirectional == True):\n",
    "            res = torch.zeros(self.num_layers_decoder*2, self.batch_size, self.hidden_size)\n",
    "        else:\n",
    "            res = torch.zeros(self.num_layers_decoder, self.batch_size, self.hidden_size)\n",
    "        res.to(device)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Attention(input_tensor, output_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size, max_length):\n",
    "    teacher_forcing_ratio = 0.5\n",
    "    # For GRU & RNN\n",
    "    encoder_hidden = encoder.initHidden(num_layers_encoder)\n",
    "    # For LSTM\n",
    "    if cell_type == \"LSTM\":\n",
    "        encoder_cell_state = encoder.initHidden(num_layers_encoder)\n",
    "        encoder_hidden = (encoder_hidden, encoder_cell_state)\n",
    "    # Transforming the shape of input_tensor to extract each of the letters one by one\n",
    "    #  from all the words from the batch.\n",
    "    input_tensor = torch.tensor(input_tensor.transpose(0, 1))\n",
    "    output_tensor = torch.tensor(output_tensor.transpose(0, 1))\n",
    "\n",
    "    # Doing gradient of encoder_optimizer & decoder_optimizer zero in the starting\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length+1, batch_size, encoder.hidden_size)\n",
    "    encoder_outputs.to(device)\n",
    "\n",
    "    loss, i = 0, 0\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    output_length = output_tensor.size(0)\n",
    "\n",
    "    # Encoder phase\n",
    "    while i < input_length:\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
    "        encoder_outputs[i] = encoder_output\n",
    "        i += 1\n",
    "\n",
    "    # Decoder phase\n",
    "    decoder_input = torch.LongTensor([SOS_token] * batch_size)\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_attention = None\n",
    "    \n",
    "    # Using the teachcer forcing ratio of 50 %.\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        i = 0\n",
    "        while i < output_length:\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_input = output_tensor[i]\n",
    "            loss += criterion(decoder_output, output_tensor[i])\n",
    "            i += 1\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        j = 0\n",
    "        while j < output_length:\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)#decoder_attention\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            loss += criterion(decoder_output, output_tensor[j])\n",
    "            j += 1\n",
    "\n",
    "    # Adjusting all the weights to reduce the loss.\n",
    "    loss.backward()\n",
    "    # updating the weights of encoder and deocder optimizer.\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    # Returing the average loss.\n",
    "    return loss.item() / output_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main function to run the training loop\n",
    "def trainIters_Attention(encoder, decoder,train_Loader, val_Loader, max_length, max_of_all, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size, n_iters, index2char):\n",
    "    # using Nadam as optimizer for learning\n",
    "    optimizer_encoder = optim.NAdam(encoder.parameters(), lr = learning_rate)\n",
    "    optimizer_decoder = optim.NAdam(decoder.parameters(), lr = learning_rate)\n",
    "    # using negative log likelihood loss to compute loss.\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        print('Epoch No : ', iter)\n",
    "        batch_no = 1\n",
    "        train_loss = 0\n",
    "        \n",
    "        for x, y in train_Loader:\n",
    "            # transferrring the x & y to gpu.\n",
    "            loss = train_Attention(x, y, encoder, decoder, optimizer_encoder, optimizer_decoder, criterion, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size, max_length)\n",
    "            train_loss += loss\n",
    "            batch_no += 1\n",
    "        print('Train Loss: ', train_loss/ len(train_Loader))\n",
    "        \n",
    "        # computing the validation accuracy and loss.\n",
    "        val_accur, val_loss = evaluate(encoder, decoder, val_Loader, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size, criterion, max_of_all, index2char)\n",
    "        print(\"Val Accuracy\", val_accur, \"Val Loss\", val_loss/len(val_Loader))\n",
    "        # storing the val_accur andd val_loss for the plots.\n",
    "        wandb.log({\"val_accuracy\" :val_accur, \"val_loss\" :val_loss/len(val_Loader), \"train_loss\" : train_loss / len(train_Loader)})\n",
    "        wandb.run.name = run_name\n",
    "        wandb.run.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sweep_with_attention():\n",
    "`    # Prepare the data.\n",
    "    # preprocessing of train , test & val dataset.\n",
    "    char_count, char2index ,index2char,maxlength_input, maxlength_output, pair_list =  preprocessData(currdir, lang_chosen, \"train\")\n",
    "    val_char_count, val_char2index, val_index2char, val_maxlength_input, val_maxlength_output, val_pair_list = preprocessData(currdir, lang_chosen, \"val\")\n",
    "    test_char_count, test_char2index, test_index2char, test_maxlength_input, test_maxlength_output, test_pair_list = preprocessData(currdir, lang_chosen, \"test\")\n",
    "    # computing the maximum length of all the words.\n",
    "    maxlength = max(maxlength_input, maxlength_output) + 2\n",
    "\n",
    "    max_of_all = max([maxlength_input, maxlength_output, val_maxlength_input, val_maxlength_output, test_maxlength_input, test_maxlength_output])\n",
    "    vec_pair_list = create_vec_pair(char2index, maxlength, pair_list)\n",
    "    val_vec_pair_list = create_vec_pair(char2index, max_of_all, val_pair_list)\n",
    "    configuration = {\n",
    "        'embedding_size' : 64,\n",
    "        'cell_type' : 'GRU', \n",
    "        'hidden_size' : 128, \n",
    "        'batch_size' : 64,\n",
    "        'bi_directional' : False,\n",
    "        'drop_out' : 0.0,\n",
    "        'num_layers' : 1\n",
    "        'learning_rate' : 0.001,\n",
    "        'dropout_encoder' : 0.2,\n",
    "        'dropout_decoder' : 0.2,\n",
    "        'epochs' : 10,\n",
    "\n",
    "    }\n",
    "    # storing all the values of hyperparameters in the appropriate variables\n",
    "    batch_size = configuration['batch_size']\n",
    "    embedding_size = configuration['embedding_size']\n",
    "    cell_type = configuration['cell_type']\n",
    "    hidden_size = configuration['hidden_size']\n",
    "    bi_directional = configuration['bi_directional']\n",
    "    drop_out = configuration['drop_out']\n",
    "    dropout_encoder = configuration['dropout_encoder']\n",
    "    dropout_decoder = configuration['dropout_decoder']\n",
    "    learning_rate = configuration['learning_rate']\n",
    "    epochs = configuration['epochs']\n",
    "\n",
    "\n",
    "    encoder = EncoderRNN_Attention(hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size, len(char2index))\n",
    "    decoder = DecoderRNN_Attention(hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size, len(char2index))\n",
    "    # Creating batches for each of the train, test & validation dataset.\n",
    "    train_loader = torch.utils.data.DataLoader(vec_pair_list, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_vec_pair_list, batch_size=batch_size, shuffle=True)\n",
    "    use_cuda = False\n",
    "    # # Training the model\n",
    "    wandb.init(project = 'deep_learning_assignment3', entity = 'cs22m081', config = configuration)\n",
    "    trainIters_Attention(encoder, decoder, train_loader, val_loader, maxlength, max_of_all, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size, epochs, index2char)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters and hyperparameters\n",
    "hidden_size = 256\n",
    "MAX_LENGTH = 10\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "lang_chosen = 'hin'\n",
    "currdir = '/kaggle/input/akshantar-original/'\n",
    "run_sweep_with_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep configuration for the attention model.\n",
    "sweep_config_attn = {\n",
    "    'method' : 'bayes',\n",
    "    \n",
    "    'metric' : {\n",
    "    'name' : 'val_accuracy',\n",
    "    'goal' : 'maximize',\n",
    "    },\n",
    "    \n",
    "    'parameters' : {\n",
    "        'epochs' : {\n",
    "            'values' : [10]\n",
    "        },\n",
    "        'bi_directional' : {\n",
    "            'values' : [True, False]\n",
    "        },\n",
    "        'cell_type' : {\n",
    "            'values' : ['RNN', 'GRU', 'LSTM']\n",
    "        },\n",
    "        'num_layers' :{\n",
    "            'values' : [1, 2, 3]\n",
    "        },\n",
    "        'hidden_size' : {\n",
    "            'values' : [128, 256, 512]\n",
    "        },\n",
    "        'batch_size' : {\n",
    "            'values' : [32, 64, 128, 256]\n",
    "        },\n",
    "        'dropout_encoder' : {\n",
    "            'values' : [0.2, 0.3, 0.4]\n",
    "        },\n",
    "        'dropout_decoder' : {\n",
    "            'values' : [0.2, 0.3, 0.4]\n",
    "        },\n",
    "        'embedding_size': {\n",
    "            'values' : [32, 64, 256, 512]\n",
    "        }\n",
    "\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run the sweeps on the above sweep configuration.\n",
    "sweep_id = wandb.sweep(sweep_config, entity=\"cs22m081\", project=\"deep_learning_assignment3\")\n",
    "wandb.agent(sweep_id, run_sweep_with_attention, count = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the infer funtion to create the prediction_attention.csv file.\n",
    "pred = infer(encoder, decoder, test_loader, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size , maxlength, index2char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
