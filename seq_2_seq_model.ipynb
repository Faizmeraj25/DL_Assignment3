{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7c042",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T11:21:57.132615Z",
     "iopub.status.busy": "2023-05-21T11:21:57.131982Z",
     "iopub.status.idle": "2023-05-21T11:22:01.773005Z",
     "shell.execute_reply": "2023-05-21T11:22:01.771996Z"
    },
    "papermill": {
     "duration": 4.653763,
     "end_time": "2023-05-21T11:22:01.775855",
     "exception": false,
     "start_time": "2023-05-21T11:21:57.122092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all the required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import unicodedata\n",
    "import string\n",
    "import os \n",
    "import pandas as pd\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6a37b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T11:22:01.793342Z",
     "iopub.status.busy": "2023-05-21T11:22:01.792958Z",
     "iopub.status.idle": "2023-05-21T11:22:01.800833Z",
     "shell.execute_reply": "2023-05-21T11:22:01.799746Z"
    },
    "papermill": {
     "duration": 0.020453,
     "end_time": "2023-05-21T11:22:01.804312",
     "exception": false,
     "start_time": "2023-05-21T11:22:01.783859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To enable gpu in the device.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f350da4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T11:22:01.822257Z",
     "iopub.status.busy": "2023-05-21T11:22:01.821878Z",
     "iopub.status.idle": "2023-05-21T11:22:01.840504Z",
     "shell.execute_reply": "2023-05-21T11:22:01.839371Z"
    },
    "papermill": {
     "duration": 0.030587,
     "end_time": "2023-05-21T11:22:01.842977",
     "exception": false,
     "start_time": "2023-05-21T11:22:01.812390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create index2char & char2index dictionary.\n",
    "# This function does preprocessing of train, test and validation path and create pairs of english & hindi words.\n",
    "def preprocessData(currdir, lang_chosen, index2char, char2index, data_type = 'train'):\n",
    "    train_path = os.path.join(currdir, lang_chosen, lang_chosen + '_train.csv')\n",
    "    val_path = os.path.join(currdir, lang_chosen, lang_chosen + '_valid.csv')\n",
    "    test_path = os.path.join(currdir,  lang_chosen, lang_chosen + '_test.csv')\n",
    "    if data_type == \"test\":\n",
    "        path = test_path\n",
    "    elif data_type == \"val\":\n",
    "        path = val_path\n",
    "    else:\n",
    "        path = train_path\n",
    "        \n",
    "    data = pd.read_csv(path, names=['input', 'output'])\n",
    "    input = data['input'].to_list()\n",
    "    output = data['output'].to_list()\n",
    "    pair_list =  []\n",
    "    # Create pairs of words.\n",
    "    for i in range(len(input)):\n",
    "        pair = (input[i], output[i])\n",
    "        pair_list.append(pair)\n",
    "        \n",
    "    # Tokens \n",
    "    # 0 -> SOS\n",
    "    # 1 -> EOS\n",
    "    # 3 -> Pad\n",
    "    index2char1 = {0:'<', 1: '>', 2 : '.'}\n",
    "    char2index1 = {'<' : 0, '>' : 1, '.' : 2 }\n",
    "    char_count = {}\n",
    "    num_char = 3\n",
    "    index = 3\n",
    "    maxlength_input = 0\n",
    "    maxlength_output = 0\n",
    "    # Creating char dictionary.\n",
    "    for word in input:\n",
    "        maxlength_input = max(maxlength_input, len(word))\n",
    "        for char in word: \n",
    "            if char not in  char2index1:\n",
    "                char2index1[char] = len(char2index1)\n",
    "                char_count[char] = 1\n",
    "                index2char1[len(index2char1)] = char\n",
    "                index = index + 1\n",
    "            else: \n",
    "                char_count[char] = char_count[char] + 1\n",
    "                \n",
    "    for word in output:\n",
    "        maxlength_output = max(maxlength_output, len(word))\n",
    "        for char in word: \n",
    "            if char not in  char2index1:\n",
    "                char2index1[char] = len(char2index1)\n",
    "                char_count[char] = 1\n",
    "                index2char1[len(char2index1)] = char\n",
    "                index = index + 1\n",
    "            else: \n",
    "                char_count[char] = char_count[char] + 1\n",
    "    # Adding in the main index2char and char2index dictionary\n",
    "    for word in input:\n",
    "        for char in word: \n",
    "            if char not in  char2index:\n",
    "                char2index[char] = len(char2index)\n",
    "                index2char[len(index2char)] = char\n",
    "                \n",
    "    for word in output:\n",
    "        for char in word: \n",
    "            if char not in  char2index:\n",
    "                char2index[char] = len(char2index)\n",
    "                index2char[len(index2char)] = char\n",
    "    return char_count, char2index, index2char, maxlength_input, maxlength_output, pair_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dec318",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T11:22:01.862007Z",
     "iopub.status.busy": "2023-05-21T11:22:01.860936Z",
     "iopub.status.idle": "2023-05-21T11:22:01.866240Z",
     "shell.execute_reply": "2023-05-21T11:22:01.865330Z"
    },
    "papermill": {
     "duration": 0.01755,
     "end_time": "2023-05-21T11:22:01.868511",
     "exception": false,
     "start_time": "2023-05-21T11:22:01.850961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create a vector for the word which contains its indices from char2index dictionary.\n",
    "def word2vec(char2index, word):\n",
    "    vec = []\n",
    "    for char in word:\n",
    "        vec.append(char2index[char])\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caceae71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T11:22:01.886546Z",
     "iopub.status.busy": "2023-05-21T11:22:01.886092Z",
     "iopub.status.idle": "2023-05-21T11:22:01.894544Z",
     "shell.execute_reply": "2023-05-21T11:22:01.893354Z"
    },
    "papermill": {
     "duration": 0.020345,
     "end_time": "2023-05-21T11:22:01.896889",
     "exception": false,
     "start_time": "2023-05-21T11:22:01.876544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a tensor for each of the words containg PAD_token, SOS_token & EOS_token and rest of the indices.\n",
    "def create_vec(char2index, maxlength, word):\n",
    "    wordvec = word2vec(char2index, word)\n",
    "    wordvec.append(EOS_token)\n",
    "    for i in range(maxlength - len(word)):\n",
    "        wordvec.append(PAD_token)\n",
    "    wordvec = torch.LongTensor(wordvec)\n",
    "    return wordvec\n",
    "# Function to create a pair of tensors of embedding of english & hindi words in a pair.\n",
    "def create_vec_pair(char2index, maxlength, pair_list):\n",
    "    vec_pair_list = []\n",
    "    for word_pair in pair_list:\n",
    "        eng_vec = create_vec(char2index, maxlength, word_pair[0])\n",
    "        hind_vec = create_vec(char2index, maxlength, word_pair[1])\n",
    "        vec_pair = (eng_vec, hind_vec)\n",
    "        vec_pair_list.append(vec_pair)\n",
    "    return vec_pair_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d738a5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T11:22:01.915509Z",
     "iopub.status.busy": "2023-05-21T11:22:01.914690Z",
     "iopub.status.idle": "2023-05-21T11:22:01.939001Z",
     "shell.execute_reply": "2023-05-21T11:22:01.938111Z"
    },
    "papermill": {
     "duration": 0.036746,
     "end_time": "2023-05-21T11:22:01.941575",
     "exception": false,
     "start_time": "2023-05-21T11:22:01.904829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Class for Encoder & Decoder for LSTM , GRU & RNN.\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, input_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding_size = embedding_size\n",
    "        self.cell_type = cell_type\n",
    "        self.embedding = nn.Embedding(input_size, self.embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "        #For RNN \n",
    "        self.cell_layer = self.cell_layer = nn.RNN(self.embedding_size, self.hidden_size, num_layers = num_layers_encoder, dropout = drop_out, bidirectional = bi_directional)\n",
    "        if self.cell_type == 'GRU':\n",
    "            self.cell_layer = nn.GRU(self.embedding_size, self.hidden_size, num_layers = num_layers_encoder, dropout = drop_out, bidirectional = bi_directional)\n",
    "        elif self.cell_type == 'LSTM':\n",
    "            self.cell_layer = nn.LSTM(self.embedding_size, self.hidden_size, num_layers = num_layers_encoder, dropout = drop_out, bidirectional = bi_directional)\n",
    " \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded.view(1,self.batch_size, -1))\n",
    "        ignored_tokens = []\n",
    "        for i in range(len(input)): \n",
    "            if i == 0:\n",
    "                ignored_tokens.append(i)\n",
    "            elif i == 1:\n",
    "                ignored_tokens.append(i+1)\n",
    "            else: \n",
    "                ignored_tokens.append(i+2)\n",
    "            if i == 2: \n",
    "                break\n",
    "                \n",
    "        output = embedded\n",
    "        output, hidden = self.cell_layer(output, hidden)\n",
    "        ignored_token.append(2)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self , num_layers):\n",
    "        if (self.bidirectional==False):\n",
    "            result = torch.zeros(num_layers, self.batch_size, self.hidden_size)\n",
    "        else:\n",
    "            result = torch.zeros(num_layers*2, self.batch_size, self.hidden_size)\n",
    "        result.to(device)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, batch_size, hidden_size , bidirectional, embedding_size, dropout, cell_type, num_layers_decoder, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # Taking the values of all the hyperparameters from the input.\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding_size = embedding_size\n",
    "        self.cell_type = cell_type\n",
    "        self.embedding = nn.Embedding(output_size, self.embedding_size)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.num_layers_decoder = num_layers_decoder\n",
    "        # For RNN\n",
    "        self.cell_layer = nn.RNN(self.embedding_size, self.hidden_size, num_layers = self.num_layers_decoder, dropout = drop_out, bidirectional = self.bi_directional)\n",
    "        # For GRU\n",
    "        if self.cell_type == 'GRU':\n",
    "            self.cell_layer =   nn.GRU(self.embedding_size, self.hidden_size, num_layers = self.num_layers_decoder, dropout = drop_out, bidirectional = self.bi_directional)\n",
    "        # For LSTM\n",
    "        elif self.cell_type == 'LSTM':\n",
    "            self.cell_layer = nn.LSTM(self.embedding_size, self.hidden_size, num_layers = self.num_layers_decoder, dropout = drop_out, bidirectional = self.bi_directional)\n",
    "        \n",
    "        # For bidirectional model\n",
    "        if (self.bidirectional == True):\n",
    "            self.out = nn.Linear(self.hidden_size*2 , output_size)\n",
    "        # For non bi-directional model\n",
    "        else:\n",
    "            self.out = nn.Linear(self.hidden_size , output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Creating embeddding and then computing output from it.\n",
    "        embedded = self.embedding(input)\n",
    "        output = self.dropout(embedded.view(1,self.batch_size, -1))\n",
    "        # Using the non-linear ReLU funciton to add non - linearity.\n",
    "        output = nn.functional.relu(output)\n",
    "        # computing output for appropriate gru, rnn or lstm.\n",
    "        output, hidden = self.cell_layer(output, hidden)\n",
    "        ignored_tokens = []\n",
    "        for i in range(len(input)): \n",
    "            if i == 0:\n",
    "                ignored_tokens.append(i)\n",
    "            elif i == 1:\n",
    "                ignored_tokens.append(i+1)\n",
    "            else: \n",
    "                ignored_tokens.append(i+2)\n",
    "            if i == 2: \n",
    "                break\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    # Function to create a hidden layer which is initialised with all zeros.\n",
    "    def initHidden(self):\n",
    "        # for Bidirectional model hidden size => (2*num_layers_decoder) x batch_size x hidden_size\n",
    "        if (self.bidirectional == True):\n",
    "            result = torch.zeros(2 * self.num_layers_decoder, self.batch_size, self.hidden_size) \n",
    "        # for non - Bidirectional model hidden size => num_layers_decoder x batch_size x hidden_size\n",
    "        else:\n",
    "            result = torch.zeros(self.num_layers_decoder , self.batch_size, self.hidden_size)\n",
    "        # Transferring the result to store on gpu.\n",
    "        result.to(device)\n",
    "        # returning the result. \n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f7882",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T11:22:01.959691Z",
     "iopub.status.busy": "2023-05-21T11:22:01.959218Z",
     "iopub.status.idle": "2023-05-21T11:22:01.974130Z",
     "shell.execute_reply": "2023-05-21T11:22:01.972924Z"
    },
    "papermill": {
     "duration": 0.027293,
     "end_time": "2023-05-21T11:22:01.976950",
     "exception": false,
     "start_time": "2023-05-21T11:22:01.949657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to train the model using teacher forcing method.\n",
    "def train(input_tensor, output_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size, max_length):\n",
    "    teacher_forcing_ratio = 0.5\n",
    "    # For GRU & RNN\n",
    "    encoder_hidden = encoder.initHidden(num_layers_encoder)\n",
    "    # For LSTM\n",
    "    if cell_type == \"LSTM\":\n",
    "        encoder_cell_state = encoder.initHidden(num_layers_encoder)\n",
    "        encoder_hidden = (encoder_hidden, encoder_cell_state)\n",
    "\n",
    "    # Transforming the shape of input_tensor to extract each of the letters one by one\n",
    "    #  from all the words from the batch.\n",
    "    input_tensor = torch.tensor(input_tensor.transpose(0, 1))\n",
    "    output_tensor = torch.tensor(output_tensor.transpose(0, 1))\n",
    "    # Doing gradient of encoder_optimizer & decoder_optimizer zero in the starting\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, batch_size, encoder.hidden_size)\n",
    "    encoder_outputs.to(device)\n",
    "\n",
    "    loss, i = 0, 0\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    output_length = output_tensor.size(0)\n",
    "\n",
    "    while i < input_length:\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
    "        i += 1\n",
    "\n",
    "    decoder_input = torch.LongTensor([SOS_token] * batch_size)\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    # Using the teachcer forcing ratio of 50 %.\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    # using Teacher forcing method.\n",
    "    if use_teacher_forcing:\n",
    "        i = 0\n",
    "        while i < output_length:\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            decoder_input = output_tensor[i]\n",
    "            loss += criterion(decoder_output, output_tensor[i])\n",
    "            i += 1\n",
    "\n",
    "    else:\n",
    "        j = 0\n",
    "        while j < output_length:\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            decoder_input = torch.cat(tuple(topi))\n",
    "\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            loss += criterion(decoder_output, output_tensor[j])\n",
    "            j += 1\n",
    "\n",
    "    \n",
    "    # Adjusting all the weights to reduce the loss. \n",
    "    loss.backward()\n",
    "    # updating the weights of encoder and deocder optimizer.\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    # Returing the average loss.\n",
    "    return loss.item() / output_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff48b4f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T11:22:01.995384Z",
     "iopub.status.busy": "2023-05-21T11:22:01.994982Z",
     "iopub.status.idle": "2023-05-21T11:22:02.011541Z",
     "shell.execute_reply": "2023-05-21T11:22:02.010382Z"
    },
    "papermill": {
     "duration": 0.028853,
     "end_time": "2023-05-21T11:22:02.014162",
     "exception": false,
     "start_time": "2023-05-21T11:22:01.985309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fuction to calculate the average accuracy and average loss of the trained model.\n",
    "def evaluate(encoder, decoder, loader, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size, criterion, max_length, index2char):\n",
    "    loss = total = correct = 0    \n",
    "    with torch.no_grad():\n",
    "        # for each of the batches in the loader, checking if each of the word is completely matching\n",
    "        #  with the predicted word or not.\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_loss = 0\n",
    "\n",
    "            encoder_hidden = encoder.initHidden(num_layers_encoder)\n",
    "            if cell_type == \"LSTM\":\n",
    "                encoder_cell_state = encoder.initHidden(num_layers_encoder)\n",
    "                encoder_hidden = (encoder_hidden, encoder_cell_state)\n",
    "\n",
    "            # Transforming input & target variable to extract each of the letter from the words of batches.\n",
    "            input_variable = batch_x.transpose(0, 1)\n",
    "            output_variable = batch_y.transpose(0, 1)\n",
    "            input_length = input_variable.size(0)\n",
    "            target_length = output_variable.size(0)\n",
    "\n",
    "            output = torch.LongTensor(target_length, batch_size)\n",
    "\n",
    "            encoder_outputs = torch.zeros(max_length, batch_size, encoder.hidden_size)\n",
    "            encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "            \n",
    "            decoder_input = torch.LongTensor([SOS_token] * batch_size)\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            for i in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(input_variable[i], encoder_hidden)\n",
    "            # passing the last output of the encoder to the fist cell of decoder. \n",
    "            decoder_hidden = encoder_hidden\n",
    "        \n",
    "            for j in range(target_length):\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "                batch_loss += criterion(decoder_output, output_variable[j].squeeze())\n",
    "\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                decoder_input = torch.cat(tuple(topi))\n",
    "                output[j] = torch.cat(tuple(topi))\n",
    "\n",
    "            output = output.transpose(0, 1)\n",
    "\n",
    "            for k in range(output.size(0)):\n",
    "                to_ignore = [SOS_token, EOS_token, PAD_token]\n",
    "                pred = []\n",
    "                y = []\n",
    "                for w in output[k]: \n",
    "                    if w not in to_ignore:\n",
    "                        y.append(index2char[w.item()])\n",
    "                \n",
    "                for w in batch_y[k]: \n",
    "                    if w not in to_ignore: \n",
    "                        pred.append(index2char[w.item()])\n",
    "                if y == pred:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "            # computing the average accuracy and loss.\n",
    "            accuracy = (correct / total) * 100\n",
    "            loss += batch_loss.item() / target_length\n",
    "    # returning accuracy and loss.\n",
    "    return accuracy, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d392f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T11:22:02.032206Z",
     "iopub.status.busy": "2023-05-21T11:22:02.031808Z",
     "iopub.status.idle": "2023-05-21T11:22:02.040995Z",
     "shell.execute_reply": "2023-05-21T11:22:02.040145Z"
    },
    "papermill": {
     "duration": 0.020852,
     "end_time": "2023-05-21T11:22:02.043143",
     "exception": false,
     "start_time": "2023-05-21T11:22:02.022291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the main function to run the training loop\n",
    "def trainIters(encoder, decoder,train_Loader, val_Loader, max_length, max_of_all, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size, n_iters, index2char):\n",
    "    # using Nadam as optimizer for learning\n",
    "    optimizer_encoder = optim.NAdam(encoder.parameters(), lr = learning_rate)\n",
    "    optimizer_decoder = optim.NAdam(decoder.parameters(), lr = learning_rate)\n",
    "    # using negative log likelihood loss to compute loss.\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        print('Epoch No : ', iter)\n",
    "        train_loss = 0\n",
    "\n",
    "        \n",
    "        for x, y in train_Loader:\n",
    "            # transferrring the x & y to gpu.\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            loss = train(x, y, encoder, decoder, optimizer_encoder, optimizer_decoder, criterion, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size, max_length)\n",
    "            train_loss += loss\n",
    "        # computing the validation accuracy and loss.\n",
    "        val_accur, val_loss = evaluate(encoder, decoder, val_Loader, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size, criterion, max_of_all, index2char)\n",
    "        print(\"Val Accuracy\", val_accur, \"Val Loss\", val_loss/len(val_Loader),'Train Loss:' , train_loss / len(train_Loader))\n",
    "        # storing the val_accur andd val_loss for the plots.\n",
    "        wandb.log({\"val_accuracy\" :val_accur, \"val_loss\" :val_loss/len(val_Loader), \"train_loss\" : train_loss / len(train_Loader)})\n",
    "    run_name = \"bs_{}_emSz_{}_nEn_{}_nDec_{}_hl_{}_ct_{}_biDir_{}_lr_{}_drp_{}\".format( configuration[\"batch_size\"], configuration['embedding_size'], configuration[\"num_layers_encoder\"], configuration[\"num_layers_decoder\"], configuration[\"hidden_size\"], configuration['cell_type'], configuration['bi_directional'], configuration['learning_rate'], configuration['drop_out'])\n",
    "    wandb.run.name = run_name\n",
    "    wandb.run.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184ac757",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T11:22:02.061889Z",
     "iopub.status.busy": "2023-05-21T11:22:02.061062Z",
     "iopub.status.idle": "2023-05-21T11:22:05.860842Z",
     "shell.execute_reply": "2023-05-21T11:22:05.858902Z"
    },
    "papermill": {
     "duration": 3.811796,
     "end_time": "2023-05-21T11:22:05.862950",
     "exception": true,
     "start_time": "2023-05-21T11:22:02.051154",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_sweep_without_attention() :\n",
    "\n",
    "    # preprocessing of train , test & val dataset.\n",
    "    char_count, char2index ,index2char,maxlength_input, maxlength_output, pair_list =  preprocessData(currdir, lang_chosen, index2char, char2index, \"train\")\n",
    "    val_char_count, char2index, index2char, val_maxlength_input, val_maxlength_output, val_pair_list = preprocessData(currdir, lang_chosen, index2char, char2index, \"val\")\n",
    "    test_char_count, char2index, index2char, test_maxlength_input, test_maxlength_output, test_pair_list = preprocessData(currdir, lang_chosen, index2char, char2index, \"test\")\n",
    "    maxlength = max(maxlength_input, maxlength_output) + 2\n",
    "    # computing the maximum length of all the words.\n",
    "    max_of_all = max([maxlength_input, maxlength_output, val_maxlength_input, val_maxlength_output, test_maxlength_input, test_maxlength_output])\n",
    "    vec_pair_list = create_vec_pair(char2index, maxlength, pair_list)\n",
    "    val_vec_pair_list = create_vec_pair(char2index, max_of_all, val_pair_list)\n",
    "    test_vec_pair_list = create_vec_pair(char2index, max_of_all, test_pair_list)\n",
    "    use_cuda = False\n",
    "    configuration = {\n",
    "        'embedding_size' : 256,\n",
    "        'cell_type' : 'LSTM',\n",
    "        'hidden_size' : 128,\n",
    "        'batch_size' : 64,\n",
    "        'bi_directional' : False,\n",
    "        'drop_out' : 0.0, \n",
    "        'num_layers_encoder' : 1,\n",
    "        'num_layers_decoder' : 1, \n",
    "        'learning_rate' : 0.001,\n",
    "    }\n",
    "    # storing all the values of hyperparameters in the appropriate variables\n",
    "    batch_size = configuration['batch_size']\n",
    "    embedding_size = configuration['embedding_size']\n",
    "    cell_type = configuration['cell_type']\n",
    "    hidden_size = configuration['hidden_size']\n",
    "    bi_directional = configuration['bi_directional']\n",
    "    drop_out = configuration['drop_out']\n",
    "    num_layers_encoder = configuration['num_layers_encoder']\n",
    "    num_layers_decoder = configuration['num_layers_decoder']\n",
    "    learning_rate = configuration['learning_rate']\n",
    "\n",
    "    encoder = EncoderRNN(hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, len(char2index))\n",
    "    decoder = DecoderRNN(batch_size, hidden_size , bidirectional, embedding_size, dropout, cell_type, num_layers_decoder, len(char2index))\n",
    "    # Creating batches for each of the train, test & validation dataset.\n",
    "    train_loader = torch.utils.data.DataLoader(vec_pair_list, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_vec_pair_list, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_vec_pair_list, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # # Trainin the model\n",
    "    wandb.init(project = 'deep_learning_assignment3', entity = 'cs22m081', config = configuration)\n",
    "    trainIters(encoder, decoder, train_loader, val_loader, maxlength, max_of_all, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size, epochs, index2char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7770231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters and hyperparameters\n",
    "hidden_size = 256\n",
    "MAX_LENGTH = 10\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "# Prepare the data.\n",
    "lang_chosen = 'hin'\n",
    "currdir = '/kaggle/input/akshantar-original/'\n",
    "# Tokens \n",
    "# 0 -> SOS\n",
    "# 1 -> EOS\n",
    "# 3 -> Pad\n",
    "index2char = {0:'<', 1: '>', 2 : '.'}\n",
    "char2index = {'<' : 0, '>' : 1, '.' : 2 }\n",
    "## calling the function to run a single time on default configuration.\n",
    "run_sweep_without_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8644825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep configuration \n",
    "sweep_config ={\n",
    "    'method':'bayes',\n",
    "    'metric' = {\n",
    "    'name' : 'validation_accuracy',\n",
    "    'goal' : 'maximize',\n",
    "     }\n",
    "   'parameters' : \n",
    "   {\n",
    "    'hidden_size':{\n",
    "        'values' : [128,256,512]\n",
    "    },\n",
    "    'learning_rate':{\n",
    "        'values' : [1e-2,1e-3]\n",
    "    },\n",
    "    'cell_type':{\n",
    "        'values' : ['LSTM','RNN','GRU']\n",
    "    },\n",
    "    'num_layers_encoder':{\n",
    "        'values' : [1,2,3]\n",
    "    },\n",
    "    'num_layers_decoder':{\n",
    "        'values' : [1,2,3]\n",
    "    },\n",
    "    'drop_out':{\n",
    "        'values' : [0.0,0.2,0.3]\n",
    "    },\n",
    "    'embedding_size':{\n",
    "        'values' : [64,128,256,512]\n",
    "    },\n",
    "    'batch_size':{\n",
    "        'values' : [32,64,128]\n",
    "    },\n",
    "    'bidirectional':{\n",
    "        'values' : [True,False]\n",
    "    }\n",
    "}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a962b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run the sweeps on the above sweep configuration.\n",
    "sweep_id = wandb.sweep(sweep_config, entity=\"cs22m081\", project=\"deep_learning_assignment3\")\n",
    "wandb.agent(sweep_id, run_sweep_without_attention, count = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c51d30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T10:47:21.513319Z",
     "iopub.status.busy": "2023-05-21T10:47:21.512885Z",
     "iopub.status.idle": "2023-05-21T10:47:52.888522Z",
     "shell.execute_reply": "2023-05-21T10:47:52.887325Z",
     "shell.execute_reply.started": "2023-05-21T10:47:21.513288Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to predict the words for each of the input in the loader and save the result in a csv file prediction.csv\n",
    "def inference(encoder, decoder, loader, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size, max_length, index2char):\n",
    "    loss = total = correct = 0    \n",
    "    predictions = {\n",
    "        \"input\" : [], \n",
    "        \"pred\" : [],\n",
    "        \"output\" : []\n",
    "    }\n",
    "    # torch.no_grad() is used to not change the weights and predict the words from the encoder-decoder model.\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            encoder_hidden = encoder.initHidden(num_layers_encoder)\n",
    "            if cell_type == \"LSTM\":\n",
    "                encoder_cell_state = encoder.initHidden(num_layers_encoder)\n",
    "                encoder_hidden = (encoder_hidden, encoder_cell_state)\n",
    "            # used to store all the input words in the given batch_x\n",
    "            input_words = [] \n",
    "            to_ignore = [SOS_token, EOS_token, PAD_token]\n",
    "            for k in batch_x:\n",
    "                input_word = \"\"\n",
    "                for kk in k: \n",
    "                    if kk not in to_ignore:\n",
    "                        input_word = input_word + index2char[kk.item()]\n",
    "                input_words.append(input_word)\n",
    "            input_variable = batch_x.transpose(0, 1)\n",
    "            output_variable = batch_y.transpose(0, 1)\n",
    "\n",
    "            input_length = input_variable.size(0)\n",
    "            target_length = output_variable.size(0)\n",
    "\n",
    "            output = torch.LongTensor(target_length, batch_size)\n",
    "\n",
    "            encoder_outputs = torch.zeros(max_length, batch_size, encoder.hidden_size)\n",
    "            encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "            \n",
    "            decoder_input = torch.LongTensor([SOS_token] * batch_size)\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            for i in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(input_variable[i], encoder_hidden)\n",
    "\n",
    "            decoder_hidden = encoder_hidden\n",
    "        \n",
    "            for j in range(target_length):\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                decoder_input = torch.cat(tuple(topi))\n",
    "                output[j] = torch.cat(tuple(topi))\n",
    "\n",
    "            output = output.transpose(0, 1)\n",
    "            # used to store all the output words and predicted words in a list from the given batch.\n",
    "            output_words = []\n",
    "            pred_words = []\n",
    "            for k in range(output.size(0)):                \n",
    "                pred_word = \"\"\n",
    "                output_word = \"\"\n",
    "                # storing the predicted word.\n",
    "                for w in output[k]: \n",
    "                    if w not in to_ignore:\n",
    "                        pred_word = pred_word + index2char[w.item()]\n",
    "                # storing the correct output word.\n",
    "                for w in batch_y[k]: \n",
    "                    if w not in to_ignore: \n",
    "                        output_word = output_word + index2char[w.item()]\n",
    "                output_words.append(output_word)\n",
    "                pred_words.append(pred_word)\n",
    "        # storing all the words from the current batch in the predictions dictionary.\n",
    "        predictions[\"input\"] = predictions[\"input\"] + input_words\n",
    "        predictions[\"pred\"]= predictions[\"pred\"] + pred_words\n",
    "        predictions[\"output\"] = predictions[\"output\"] + output_words\n",
    "\n",
    "    predict = pd.DataFrame(predictions)\n",
    "    # creating a csv file.\n",
    "    predict.to_csv(\"prediction.csv\")\n",
    "    return predict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565594c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the infer funtion to create the csv file.\n",
    "pred = infer(encoder, decoder, test_loader, hidden_size, batch_size, bidirectional, embedding_size, dropout, cell_type, num_layers_encoder, num_layers_decoder, learning_rate, embedding_size , maxlength, index2char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22.47819,
   "end_time": "2023-05-21T11:22:07.595000",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-21T11:21:45.116810",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
